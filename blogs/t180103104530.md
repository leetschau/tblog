# PySpark Environment on Ubuntu 16.04
Tags: pyspark; ubuntu

------

This environment contains Python development tools of Spark application:
miniconda, ipython; updb; spark libraries, etc.

# Build the environment

First download and install miniconda, then:

Option 1: put all dependencies in a conda env:
```
conda create -n portablePySpark python=3.5 ipython
conda install -n portablePySpark -c conda-forge pyspark
conda install -n portablePySpark -c conda-forge pudb
conda install -n portablePySpark -c cyclus java-jdk
```

Option 2: use an external Spark.

Instead of installing `pyspark` with conda, download and extract a Spark binary
package (here it's *spark-2.2.0-bin-hadoop2.7.tgz* and extracted into folder
`$HOME/apps`), add the following commands into `~/.zshenv`:
```
export SPARK_HOME=$HOME/apps/spark-2.2.0-bin-hadoop2.7
export PYTHONPATH="${SPARK_HOME}/python/:$PYTHONPATH"
export PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH"
```

# Verify
```
cat << EOF > demo.py
from pyspark.sql.session import SparkSession
from pyspark.sql.types import FloatType
spark = SparkSession.builder.master("local[*]").appName("demo").getOrCreate()
mylist = [1.0, 2.3, 3.4]
df = spark.createDataFrame(mylist, FloatType())
df.show()
EOF

. activate portablePySpark
python demo.py
```

# Start Local PySpark Environment
```
cat << EOF > $HOME/.local/bin/startPySparkEnv
#!/bin/zsh
export SPARK_HOME=$HOME/apps/spark-2.2.0-bin-hadoop2.7
export PYTHONPATH="${SPARK_HOME}/python/:$PYTHONPATH"
export PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH"
source $HOME/.sdkman/bin/sdkman-init.sh
source activate portablePySpark
EOF
```

Start the environment with `. startPySparkEnv`.
