# Using Anaconda in Spark
Tags: anaconda; spark; pyspark; dataframe; pandas

------

To use packages (pandas, numpy, statsmodels, etc) of Anaconda in Spark,
sumbit spark job (or start spark shell) in anaconda environment.

The following codes demonstrate convert dataframe between Spark and pandas:
```
$ export SPARK_HOME=$HOME/apps/spark-2.2.0-bin-hadoop2.7
$ export PATH=$SPARK_HOME/bin:$PATH
$ . activate anaconda
$ ipython
from pyspark.sql import SparkSession
from pandas import Series
spark = SparkSession.builder.master("local[*]").appName('demo').getOrCreate()
spdf = spark.createDataFrame([(3, 33), (4, 44)])
spdf.printSchema()
pdf = spdf.toPandas()
pdf['new'] = Series([1.1, 2.3])
pdf.info()
fdf = spark.createDataFrame(pdf)
```

